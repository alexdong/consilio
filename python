from functools import partial
from pathlib import Path
from typing import Any, Callable, Optional, TypeVar, List
import json
import click
from consilio.models import Topic, BaseModel
from consilio.utils import get_llm_response

T = TypeVar('T', bound=BaseModel)

def save_response(response: List[T], file: Path) -> None:
    """Generic response saver for model objects"""
    json_str = json.dumps([r.model_dump() for r in response], indent=2)
    file.write_text(json_str)

def handle_command_flow(
    topic: Topic,
    build_prompt_fn: Callable[[Topic, str], str],
    input_file: Optional[Path] = None,
    response_definition: Optional[Any] = None,
    display_fn: Optional[Callable[[List[T]], None]] = None,
    get_template_fn: Optional[Callable[[], str]] = None,
    response_file: Optional[Path] = None,
) -> None:
    """Generic command flow handler
    
    Args:
        topic: The current Topic instance
        build_prompt_fn: Function to build the prompt for LLM
        input_file: Optional path to read/write user input
        response_definition: Type definition for LLM response
        display_fn: Optional function to display the response
        get_template_fn: Optional function to get template for new input file
        response_file: Optional path to save the response
    """
    # Handle input
    user_input = ""
    if input_file:
        if input_file.exists():
            user_input = input_file.read_text()
        else:
            template = get_template_fn() if get_template_fn else ""
            user_input = click.edit(text=template)
            assert user_input is not None
            input_file.write_text(user_input)

    # Get and handle response
    prompt = build_prompt_fn(topic, user_input)
    response = get_llm_response(prompt, response_definition)
    
    if display_fn:
        display_fn(response)
    
    if response_file:
        save_response(response, response_file)
        click.echo(f"Generated response saved to: {response_file}")
